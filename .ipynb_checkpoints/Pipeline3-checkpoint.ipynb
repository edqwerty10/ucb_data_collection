{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for flow processing\n",
    "\n",
    "## TO DO EDSON: do the to do 0, 1, ..., 8\n",
    "## Also, for every function, add explicitly the input files as parameters of the function. And cleary state the outputs of the function.\n",
    "\n",
    "### Once, this is done: process 2015 flow data, rewrite \"put years together\", run PCA and create heatmap with ArcGIS (heatmap will be done by Theo). \n",
    "\n",
    "**TO DO 0**: Explain the goal of this process here. <br>\n",
    "**Edson Question**: Not sure what to explain in this section. What is the \"process\" here? Am I suppose to turn the 3 bullet points into a paragraph?\n",
    "***Theo Answer***: the 3 bullet points can be let like this. Maybe you can explain where the input comes from and what you can derive for them, and quickly explain the structure of the notebook (one paragraph to explain 1,2,3 and 4 sections).\n",
    "\n",
    "- Process the flow data to files that are easy to read\n",
    "- Associate every detector to a network link inside Aimsun\n",
    "- Associate every detector to a road section to create heatmap and compare flows between years\n",
    "\n",
    "Explain the inputs:\n",
    "- PeMS account\n",
    "- City flow data\n",
    "- Kimley-Horn flow and speed data\n",
    "\n",
    "**New to do**: should have exception handler in the process (and python function) if the files are not located where you are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining Data\n",
    "\n",
    "### A) PeMS Data Download\n",
    "Source file code: PeMS Download.ipynb <br>\n",
    "Download traffic data from the PEMS website (pews.dot.ca.gov) for years 2013, 2017 and 2019 <br>\n",
    "- **TODO (very) later**:\n",
    "Currently, we did not succeed to give our PeMS credentials in the Python script. So we are logging to PeMS in a browser and download every file by opening a new tab in the same browser (using jupyter in the same browser). At the end, all the downloaded files are in the download folder.\n",
    "We could improve this ideally by not having to open tabs to download excel files. Also, the url to download the files was written almost manually (in the time_for_year function).\n",
    "\n",
    "We download the data calling the method download(detector_ids, year) from the pems_download.py file. The process takes about 5 minutes to run. \n",
    "\n",
    "The function has for input:\n",
    "- PeMS detectors ID\n",
    "- Year for the desired data \n",
    "\n",
    "The function has for output:\n",
    "- All corresponding PeMS detectors data file for the given year (and the given days encoded in the url)\n",
    "- Stored in the download folder as PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
    "\n",
    "For the function to work, you need to:\n",
    "- Log in to PeMS in the same browser that runs this Jupyter notebook\n",
    "\n",
    "For the pipeline to work:\n",
    "- The downloaded files are then manually moved to the folders PeMS/PeMS_YEAR (where YEAR=2013,2017 or 2019).\n",
    "\n",
    "### B) Turn city pdf to doc\n",
    "\n",
    "Some of the flow data that we got from the city were in pdf files. To be able to parse them, we convert them to doc files using online website.\n",
    "\n",
    "### C) Location and structure of the files\n",
    "\n",
    "#### PeMS files\n",
    "The PeMS files are in a folder PeMS/PeMS_YEAR/ (where YEAR=2013, 2017 or 2019).\n",
    "Every file is named PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
    "\n",
    "One xlsx file has two sheets:\n",
    "- PeMS Report Description\n",
    "- Report Data <br>\n",
    "    - Contains the traffic flow data\n",
    "    - Each row gives the number of vehicles observed in one time step (5 minutes) per lane number over the columns.\n",
    "    - The first column gives the date and time stamp, and the columns that follow are lanes (i.e. Lane 1 Flow, Lane 2 Flow)\n",
    "    - ***Edson Question***: There are columns that are ambiguous \"Flow (Veh/5 Minutes)\", \"# Lane Points\" and \"% Observed\". That is I don't know what they represent exactly. For example, \"Flow (Veh/5 Minutes)\" does not specify if it belongs to some Lane or if its a combination of the previous lane flows.\n",
    "    ***Theo Answer***: what matter is \"Flow (Veh/5 Minutes)\": the number of vehicles seen for every lane of the corresponding detectors and \"% Observed\": how much of the flow is due to real vehicles sensed or due to estimation form other days due to a technical issue that make the sensor not sensing every cars. \n",
    "\n",
    "#### City files\n",
    "\n",
    "The city files are in folder named \"Year ADT Data\" where Year takes values of 2013, 2017 or 2019. Almost every file is named using the convention \"Main Road Cross 1 Cross 2 Direction\". Main Road is the road on which the flow is recorded, Cross 1 and Cross 2 can be used to locate the sensor which is found between the intersection of the main road and road Cross 1 and the intersection of the main road and road Cross 2, and finally the Direction gives the direction of the flow such as \"EB\" which stands for East Bound. Note that files in \"Year ADT Data\" folder are of file type .xls, .xlsx, .csv and .pdf.\n",
    "\n",
    "- ***2013 Excel files*** are structured in data sheets. The first data sheet \"Summary\" contains the main road, cross streets, city information and the start date of the recording. It also summarizes the data contained in all other sheets into a bar plot of traffic flow vs time of day bins (i.e Tuesday AM, Wednesday PM) for different flow directions and into a line plot of traffic flow vs. hour of day for different days of the week. The sheets that follow are named \"D1\", \"D2\",...\"DN\" where N denotes the N'th day since the start date. These sheets are structured into two tables, AM counts and PM counts. Each table row gives the traffic flow per timestep of 15 minutes. The first column is the time of day in hh:mm format follow by direction columns of traffic flow (NB, SB, EB, WB).\n",
    "***Theo remark***: they are hidden sheets! This is very important for the parsing!\n",
    "\n",
    "- ***2017 Excel files*** are structured in one data sheet giving a header and a table for traffic flow. The header gives the start date and time of the recording, site code and sensor location, and the table gives traffic flow per a 15 minute timestep. The table's first two columns give the date and time and the following columns give traffic flow per directions.\n",
    "\n",
    "- ***2017 PDF files*** are structured with a header and 3 tables of traffic flow data (one table per day of subsequent days). The header gives the site location and other miscellaneous meta data. Each table is titled by the date and timestep (15 minutes) of the recording. A table is organized by columns each representing the hour of day (0 - 23). Hence for a given column, the first row gives the hour of the day, the second gives the total flow for the hour, and the third to last row (4 rows total) gives traffic flow per 15 minute timestep for the hour.\n",
    "\n",
    "- ***2019 Excel files*** have similar structure as those of 2013. The data is organized in two types of sheet, \"Day N\" and \"GR N\" sheets. The \"Day N\" sheets give traffic flow data in the same fashion as the \"DN\" sheets of 2013 excel files. The day of recording can be found in the header of the two tables. The \"GR N\" sheets plot the corresponding flow data of the \"Day N\" sheets. A line plot of flow vs. hour of day for different flow directions is given.\n",
    "***Theo remark***: Same here, they are hidden sheets!\n",
    "\n",
    "- ***2019 PDF files*** have the same structure as those of 2017. \n",
    "\n",
    "**TO DO LATER**: Same for the 2015 Kimley Horn flow and speed data.\n",
    "\n",
    "**To do Edson**: Add 2019 doc files and 2017 doc files. Explain that they are in folder \"Year doc Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pems_download as pems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_ids = [403250, 403256, 403255, 403257, 418387, 418388, 400376,\n",
    "#                413981, 413980, 413982, 402794, 413983, 413984, 413985,\n",
    "#                413987, 413986, 402796, 413988, 402799, 403251, 403710,\n",
    "#                403254, 403719, 400566, 418420, 418419, 418422, 418423,\n",
    "#                402793, 403226, 414015, 414016, 402795, 402797, 414011,\n",
    "#                402798]\n",
    "# pems.download(detector_ids, 2013)\n",
    "# pems.download(detector_ids, 2019)\n",
    "# pems.download(detector_ids, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDs of the PeMS detectors where obtained using ArcGIS software and an input file, pems_detectors.csv, containing the locations of all the PeMS dectectors in California <br>\n",
    "***Edson Question:*** Not sure if this suffices. Who to ask to know more about this process?\n",
    "***Theo Answer:*** please ask me! We downloaded the list of all detectors and corresponding location from PeMS, then we perform a selection in ArcGIS to find the one in the area of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing city data\n",
    "Source file code: Pre-processing_flow.ipynb <br>\n",
    "\n",
    "**To do**: The time format should be the same everywhere after the processing\n",
    "\n",
    "### A) Parse the city data from xlsx, doc and csv files to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process ADT Data files to csv files\n",
    "Here we process the Excel ADT data files (city data) into CSV files. We do this by calling the function process_adt_data(year) from the pre_process_flow.py python file. Note that one file corresponds to one main road and the traffic flow data recordings in it.\n",
    "\n",
    "Description of function process_adt_data(year) <br>\n",
    "The function has input:\n",
    "- Year which takes values of 2013, 2017 and 2019\n",
    "- Excel files located in \"Year ADT Data\" where Year=2013, 2017 or 2019\n",
    "\n",
    "The function has output:\n",
    "- CSV files located in \"Year processed\" where Year=2013, 2017 or 2019\n",
    "\n",
    "For the function to work:\n",
    "- Excel files must be located in \"Year ADT Data\" folder\n",
    "\n",
    "For the pipeline to work:\n",
    "- 2013 CSV files are manually relocated to \"City/2013 reformat\"\n",
    "- 2017 and 2019 CSV files are manually relocated to \"City/Year reformat/Format from xlsx\" where Year=2017 or 2019\n",
    "\n",
    "**(Done) TO DO 6**: Go over the code, comment it, make it easier to understand. State clearly the inputs and the outputs (this will help a lot for 2015 data). Ask Zixuan the work she did for 2015 data.\n",
    "<br>\n",
    "***Edson Question***: Code rewritten so that it is easier to understand. Input and outputs stated for large code segments in python files and methods being called here. Code comments made per code chunk as needed. I asked Zixuan for 2015 data, can I get access to the dropbox holding 2015 data?. She said she processed the speed and flow for 2015 data.\n",
    "***Theo Answer***: I send you the info on Slack\n",
    "\n",
    "***Theo Remark***: Maybe we should here put \"Year ADT Data\" and \"Year processed\" as parameters of the function process_adt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre_process_flow as pre_process\n",
    "\n",
    "# pre_process.process_adt_data(2013)\n",
    "# pre_process.process_adt_data(2017)\n",
    "# pre_process.process_adt_data(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word doc data pre-processing\n",
    "We process the word DOC files into CSV files by calling the function process_doc_data(year) from the pre_process.py python file. Note that year 2013 did not have DOC files.\n",
    "\n",
    "Description of function process_doc_data(year) <br>\n",
    "The function has input:\n",
    "- Year which takes values 2017 or 2019\n",
    "- Word DOC files located in the \"Year doc\" folder where Year=2017 or 2019\n",
    "\n",
    "The function has output:\n",
    "- CSV files located in the \"Year processed\" folder where Year=2017 or 2019\n",
    "\n",
    "For function to work:\n",
    "- DOC files must be located in \"Year doc\" folder where Year=2017 or 2019\n",
    "\n",
    "For pipeline to work:\n",
    "- CSV files must be manually relocated to \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_process.process_doc_data(2017)\n",
    "# pre_process.process_doc_data(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**:\n",
    "- The following doc files from 2019 were not processed due to the textract package not parsing it correctly. The can be found on the 2019 error folder.\n",
    "    - DURHAM RD BT I-680 AND MISSION BLVD EB.doc\n",
    "    - MISSION BLVD BT WASHINGTON BLVD AND PINES ST SB.doc\n",
    "    \n",
    "Theo: I might have done some work manually here to create the two corresponding csv files. In this case explain the process done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Find the coordinates of the city detectors\n",
    "We obtained the coordinates of the detectors by calling the method get_deo_data(year) from the pre_process.py python file. Internally, it iterators over the ADT files and obtains the adresses of the detectors to then use with Google API to obtain latitude and longitude coordinates\n",
    "\n",
    "Description of get_geo_data(year) <br>\n",
    "The function has input:\n",
    "- Year takes values 2013, 2017, 2019\n",
    "- ADT files (Excel, PDF) located in \"Year ADT Data\"\n",
    "\n",
    "The function has output:\n",
    "- \"year_info_coor.csv\" containing the coordinates of detectors\n",
    "\n",
    "For the function to work:\n",
    "- ADT files must be located in \"Year ADT Data\" folder\n",
    "\n",
    "For the pipeline to work:\n",
    "- NA\n",
    "\n",
    "**(DONE) TO DO 7**: make sure that the \"parsing data\" code create year_info_coor.csv. I think that I have created the files using some bash scripts and some excel functions. Here is the google doc that we used for the process of the flow. https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0\n",
    "Also this step is very important because this is where I gave an ID to every detector. Please check this step with Theo, to write out the process. We might need to write a function inside Python (instead of bash + Excel).\n",
    "<br>\n",
    "***Edson***: Geo data is now obtained through parsing of the file data or the file name. That is, the address is obtained and then used with google API to get Latitude and Longitude.\n",
    "\n",
    "***Theo remark***: Theo and Edson should discuss about that to get the process right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_process.get_geo_data(2013)\n",
    "# pre_process.get_geo_data(2017)\n",
    "# pre_process.get_geo_data(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS <br>\n",
    "Done in the software manually in ArcGIS.\n",
    "1. Export Aimsun network as GIS file\n",
    "2. Import Aimsun network in ArcGIS\n",
    "3. Import detectors in ArcGIS as XY_points\n",
    "4. Move detectors to put them on corresponding road in Aimsun\n",
    "5. Associate to every detectors the External ID of the Aimsun road (to be done again)\n",
    "\n",
    "**TO DO THEO**: Add the process to create the detectors inside Aimsun.\n",
    "Add the process to match the detectors to road section (and create the file lines_to_detectors.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_flow as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Process the csv files (city + caltrans) to one big file. <br>\n",
    "source code: processing flow to one CSV.ipynb <br>\n",
    "\n",
    "We combine all the flow traffic data into one big CSV file from both city and PeMS data files. This is done by calling the function process_data() from the process_flow.py python file. \n",
    "\n",
    "Description of function process_data()\n",
    "<br>\n",
    "The function has input:\n",
    "- \"Flow_processed_tmp.csv\" file that lists all the processed files from city and PeMS data\n",
    "- The processed files created from the Parsing Data section\n",
    "\n",
    "The function has output:\n",
    "- \"Flow_processed_city.csv\" containing combined city flow data for all year\n",
    "- \"Flow_processed_PeMS.csv\" containing combined PeMS flow data for all years\n",
    "\n",
    "For the function to work:\n",
    "- The processed files (input) must be located in City and PeMs folders\n",
    "- 2013 city processed files are located in \"City/2013 reformat/\"\n",
    "- 2017 and 2019 city processed files that originated from DOC (which originated from PDF) files are located in \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019\n",
    "- 2017 and 2019 city processed files that originated from Excel files are located in \"City/Year reformat/Format from xlsx\" folder where Year=2017 or 2019\n",
    "- 2013, 2017 and 2019 PeMS data files are located in \"PeMS_Year\" folder where Year=2013, 2017 or 2019\n",
    "\n",
    "For the pipeline to work:\n",
    "- The ouput files must remain in the working directory, no moving necessary.\n",
    "\n",
    "Structure of ouput files: \n",
    "- Flow_processed_city.csv\n",
    "    - contains city traffic data where the rows represent traffic flow. The first 5 columns give info about the traffic flow and are Year, Name, Id, Direction, Day 1 where Name refers to the file name from which the data originated, Id is the Id from the \"Flow_processed_tmp.csv\" file, Direction is the direction of flow and Day 1 is the start date of recording. The columns that follow are day-timesteps for flow data. There are 3 days total over which traffic flow is recorded and time progresses in 15 minute steps. Hence the data columns progress as \"Day 1 - 0:0\", \"Day 1 - 0:15\", \"Day 1 - 0:30\",...,\"Day 3 - 23:30\", \"Day 3 - 23:45\".\n",
    "- Flow_processed_PeMS.csv\n",
    "    - contains PeMS flow traffic data where the rows represent traffic flow. The first columns are Name, Id and Name PeMS where Name contains the PeMS detector Id, Id is the Id assigned from \"Flow_processed_tmp.csv\", Name PeMs is the road address. The next 6 columns give Observed Year and Day Year for the 3 years, 2013, 2017 and 2019. Observed Year is the percentage of the observed data and Day Year is the start date of recording. The columns that follow are Year-Day-timestep, there are 3 years, 3 days and time progresses in 15 minute steps. Hence the columns progress as \"2013-Day 1 - 0:0\", \"2013-Day 1 - 0:15\", \"2013-Day 1 - 0:30\",...,\"2019-Day 3 - 23:30\", \"2019-Day 3 - 23:45\".\n",
    "\n",
    "**(DONE) TO DO 8**: Explain the structure of the output files. Also, feel free to document the doc in the python file (or iPython file). Explain also the input (Flow_processed_tmp.csv) and how it was created (I think it was created from the google doc https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0).\n",
    "\n",
    "***Edson Question***: the Flow_processed_tmp.csv file and the google doc seem the same to me (except for the lat, lng info on the right side of the google doc). Beyond this, I don't know how the file was created. Who to ask for more info?\n",
    "\n",
    "***Theo Answer***: I guess I have created the file from the google doc. But I have also created the google doc during the parsing. Probably in 2) we can create Flow_processed_tmp.csv from year_info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf.process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Create file that gives traffic flows for specific road sections for every year. <br>\n",
    "source note: put years together.ipynb\n",
    "\n",
    "- Use detectors (lines_to_detectors.csv) and flow processed city (flow_processed_city.csv) data to create all years flow data in \"flow_processed_section.csv\"\n",
    "- Note that the erroneous files are still being skipped, they are: ['DurhamRd I680 MissionBlv EB', 'Mission blvd Pine Washington SB']\n",
    "\n",
    "**To do 9**: the function pytogether should be written again.\n",
    "- add the PeMS data\n",
    "- be more clever about missing data or road section associated with several detectors (take the average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_years_together as pytogether\n",
    "\n",
    "line_to_detectors = 'lines_to_detectors.csv'\n",
    "flow_processed_city = 'Flow_processed_city.csv'\n",
    "pytogether.run(line_to_detectors, flow_processed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Analysis\n",
    "\n",
    "**TO DO 10**: To be done after the other to dos has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Analyse PCA results using heatmap inside ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
